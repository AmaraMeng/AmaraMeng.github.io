<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.24" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.95" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme="dark"] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"01. 基于EEG的卷积神经网络听觉注意力定位检测","image":[""],"datePublished":"2025-11-23T11:39:12.000Z","dateModified":"2025-11-25T07:54:40.000Z","author":[{"@type":"Person","name":"Ran"}]}</script><meta property="og:url" content="https://pythiaroot.com/reading/literature/01-EEG-based-detection-of-the-locus-of-auditory-attention-with-convolutional-neural-networks.html"><meta property="og:site_name" content="Pythia’s Root"><meta property="og:title" content="01. 基于EEG的卷积神经网络听觉注意力定位检测"><meta property="og:description" content="EEG-based detection of the locus of auditory attention with convolutional neural networks 题目及作者题目及作者 摘要 In a multi-speaker scenario, the human auditory system is able to attend ..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-11-25T07:54:40.000Z"><meta property="article:author" content="Ran"><meta property="article:published_time" content="2025-11-23T11:39:12.000Z"><meta property="article:modified_time" content="2025-11-25T07:54:40.000Z"><link rel="icon" href="/logo.jpg"><title>01. 基于EEG的卷积神经网络听觉注意力定位检测 | Pythia’s Root</title><meta name="description" content="EEG-based detection of the locus of auditory attention with convolutional neural networks 题目及作者题目及作者 摘要 In a multi-speaker scenario, the human auditory system is able to attend ...">
    <link rel="preload" href="/assets/style-fescsFu7.css" as="style"><link rel="stylesheet" href="/assets/style-fescsFu7.css">
    <link rel="modulepreload" href="/assets/app-CyVX4-jx.js"><link rel="modulepreload" href="/assets/01-EEG-based-detection-of-the-locus-of-auditory-attention-with-convolutional-neural-networks.html-C3NhXUPS.js"><link rel="modulepreload" href="/assets/plugin-vue_export-helper-DlAUqK2U.js">
    <link rel="prefetch" href="/assets/index.html-CpUmG-lr.js" as="script"><link rel="prefetch" href="/assets/intro.html-C_F3KPrc.js" as="script"><link rel="prefetch" href="/assets/Cao-jing-JYM.html-B2w2L0HT.js" as="script"><link rel="prefetch" href="/assets/01-Ch1.html-EvPKYtI_.js" as="script"><link rel="prefetch" href="/assets/01-basic-math-knowledge.html-UlJ0o-BM.js" as="script"><link rel="prefetch" href="/assets/00-python-tips.html-CXqcc-1_.js" as="script"><link rel="prefetch" href="/assets/01-variable.html-B6SwjOw1.js" as="script"><link rel="prefetch" href="/assets/02-data-type.html-DH2siOos.js" as="script"><link rel="prefetch" href="/assets/02-test-variable-datatype.html-BWBdkRTE.js" as="script"><link rel="prefetch" href="/assets/03-numeric-type.html-Bfl0ut4_.js" as="script"><link rel="prefetch" href="/assets/04-string.html-F-0mBm5n.js" as="script"><link rel="prefetch" href="/assets/05-list.html-DKvuUVZK.js" as="script"><link rel="prefetch" href="/assets/06-tuple.html-DpBatMWK.js" as="script"><link rel="prefetch" href="/assets/07-dict.html-C5jVw3VS.js" as="script"><link rel="prefetch" href="/assets/08-set.html-CJ06gQvR.js" as="script"><link rel="prefetch" href="/assets/09-bool.html-BxB-JBjp.js" as="script"><link rel="prefetch" href="/assets/10-if.html-CVQALABx.js" as="script"><link rel="prefetch" href="/assets/11-while.html-DO4AUSYl.js" as="script"><link rel="prefetch" href="/assets/12-for.html-6hCQ3y0E.js" as="script"><link rel="prefetch" href="/assets/13-functions.html-V3uMxasi.js" as="script"><link rel="prefetch" href="/assets/14-class.html-NhP6ZVn2.js" as="script"><link rel="prefetch" href="/assets/15-environment.html-BQlOA1a3.js" as="script"><link rel="prefetch" href="/assets/16-python3-errors-and-exceptions.html-Duu5MksH.js" as="script"><link rel="prefetch" href="/assets/17-TBSgame.html-BNcUfTQv.js" as="script"><link rel="prefetch" href="/assets/18-local-LLM.html-D6psryBV.js" as="script"><link rel="prefetch" href="/assets/19-python-install.html-DeqdfKZq.js" as="script"><link rel="prefetch" href="/assets/20-anaconda-install.html-aH3Kl9CO.js" as="script"><link rel="prefetch" href="/assets/Markdown-formula.html-D8NNF_Wv.js" as="script"><link rel="prefetch" href="/assets/Static-website-building.html-Cl2NYTdz.js" as="script"><link rel="prefetch" href="/assets/article-typora.html-C1Jb-gng.js" as="script"><link rel="prefetch" href="/assets/01-LOVEintheTIMEofCHOLERA.html-B-gTXeNV.js" as="script"><link rel="prefetch" href="/assets/02-Tuesdays-with-Morrie.html-Bl6VykGd.js" as="script"><link rel="prefetch" href="/assets/01-human-DK10000.html-CIt5E8gL.js" as="script"><link rel="prefetch" href="/assets/404.html-CTqs7f0H.js" as="script"><link rel="prefetch" href="/assets/index.html-StLphS9o.js" as="script"><link rel="prefetch" href="/assets/index.html-BdX545S2.js" as="script"><link rel="prefetch" href="/assets/index.html-C6jcEwOW.js" as="script"><link rel="prefetch" href="/assets/index.html-DQgp5H5U.js" as="script"><link rel="prefetch" href="/assets/index.html-LZKnjP1c.js" as="script"><link rel="prefetch" href="/assets/index.html-BS-gvSCq.js" as="script"><link rel="prefetch" href="/assets/index.html-DQx3g-xC.js" as="script"><link rel="prefetch" href="/assets/index.html-B8zPY6JQ.js" as="script"><link rel="prefetch" href="/assets/index.html--tfnLEIQ.js" as="script"><link rel="prefetch" href="/assets/index.html-DxBSE_KG.js" as="script"><link rel="prefetch" href="/assets/index.html-DqKrnJ_2.js" as="script"><link rel="prefetch" href="/assets/index.html-6NSxz4C4.js" as="script"><link rel="prefetch" href="/assets/index.html-6UDusiGF.js" as="script"><link rel="prefetch" href="/assets/index.html-BWo5F6Iy.js" as="script"><link rel="prefetch" href="/assets/index.html-Dip0LcVD.js" as="script"><link rel="prefetch" href="/assets/index.html-C2HDKzfk.js" as="script"><link rel="prefetch" href="/assets/index.html-dHsrLrBH.js" as="script"><link rel="prefetch" href="/assets/index.html-BBkb1v6I.js" as="script"><link rel="prefetch" href="/assets/index.html-DodakFax.js" as="script"><link rel="prefetch" href="/assets/index.html-BFEIMkYz.js" as="script"><link rel="prefetch" href="/assets/index.html-IsDks7nE.js" as="script"><link rel="prefetch" href="/assets/index.html-BgeZCbSD.js" as="script"><link rel="prefetch" href="/assets/index.html-4uxHev5g.js" as="script"><link rel="prefetch" href="/assets/index.html-BdgOJGWd.js" as="script"><link rel="prefetch" href="/assets/mermaid.esm.min-CIY8jxlu.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-CKV1Bsxh.js" as="script"><link rel="prefetch" href="/assets/waline-meta-l0sNRNKZ.js" as="script"><link rel="prefetch" href="/assets/component-IdeOYIqk.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container external-link-icon has-toc" vp-container><!--[--><header id="navbar" class="vp-navbar" vp-navbar><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><a class="route-link vp-brand" href="/" aria-label="带我回家"><img class="vp-nav-logo" src="/阿尔法 logo.jpg" alt><!----><span class="vp-site-name hide-in-pad">Pythia’s Root</span></a><!--]--></div><div class="vp-navbar-center"><!--[--><nav class="vp-nav-links"><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/" aria-label="Pythia’s Root"><!--[--><i class="vp-icon iconfont icon-house" sizing="height"></i><!--]-->Pythia’s Root<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="编程"><!--[--><i class="vp-icon iconfont icon-biancheng-01" sizing="height"></i>编程<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/programming/static-website-blog/" aria-label="Static website building-blog"><!--[--><i class="vp-icon iconfont icon-boke" sizing="both"></i><!--]-->Static website building-blog<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/programming/python/" aria-label="Python"><!--[--><i class="vp-icon iconfont icon-python" sizing="both"></i><!--]-->Python<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/programming/ThinkDSP/" aria-label="ThinkDSP"><!--[--><i class="vp-icon iconfont icon-python" sizing="both"></i><!--]-->ThinkDSP<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/programming/math-for-ai/" aria-label="Math for AI"><!--[--><i class="vp-icon iconfont icon-python" sizing="both"></i><!--]-->Math for AI<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/#" aria-label="人工耳蜗"><!--[--><i class="vp-icon iconfont icon-rengongerwoshenqing" sizing="height"></i><!--]-->人工耳蜗<!----></a></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/#" aria-label="语音学"><!--[--><i class="vp-icon iconfont icon-shengboyuyinxiaoxi" sizing="height"></i><!--]-->语音学<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="翻译"><!--[--><i class="vp-icon iconfont icon-fanyi" sizing="height"></i>翻译<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link auto-link" href="/translation/vocabulary/" aria-label="词汇"><!--[--><i class="vp-icon iconfont icon-cihuiben" sizing="both"></i><!--]-->词汇<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/translation/peki/" aria-label="Peki"><!--[--><i class="vp-icon iconfont icon-fanyi" sizing="both"></i><!--]-->Peki<!----></a></li></ul></button></div></div><div class="vp-nav-item hide-in-mobile"><a class="route-link auto-link" href="/sports/" aria-label="运动"><!--[--><i class="vp-icon iconfont icon-jianshenfang" sizing="height"></i><!--]-->运动<!----></a></div><div class="vp-nav-item hide-in-mobile"><div class="vp-dropdown-wrapper"><button type="button" class="vp-dropdown-title" aria-label="阅读"><!--[--><i class="vp-icon iconfont icon-yuedu" sizing="height"></i>阅读<!--]--><span class="arrow"></span><ul class="vp-dropdown"><li class="vp-dropdown-item"><a class="route-link route-link-active auto-link" href="/reading/literature/" aria-label="文献"><!--[--><i class="vp-icon iconfont icon-yuedu" sizing="both"></i><!--]-->文献<!----></a></li><li class="vp-dropdown-item"><a class="route-link auto-link" href="/reading/books/" aria-label="书籍"><!--[--><i class="vp-icon iconfont icon-yuedu" sizing="both"></i><!--]-->书籍<!----></a></li></ul></button></div></div></nav><!--]--></div><div class="vp-navbar-end"><!--[--><!----><div class="vp-nav-item vp-action"><a class="vp-action-link" href="https://github.com/AmaraMeng/AmaraMeng.github.io" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" name="github" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="vp-nav-item hide-in-mobile"><button type="button" class="vp-color-mode-switch" id="color-mode-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" name="auto" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" name="dark" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" name="light" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" placeholder="搜索" autocomplete="off" spellcheck="false" value><!----></form><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar" vp-sidebar><!----><ul class="vp-sidebar-links"><li><a class="route-link auto-link vp-sidebar-link" href="/" aria-label="Pythia’s Root"><!--[--><i class="vp-icon iconfont icon-house" sizing="both"></i><!--]-->Pythia’s Root<!----></a></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Programming</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">Reading</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Books</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable active" type="button"><!----><span class="vp-sidebar-title">Literature</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><a class="route-link route-link-active auto-link vp-sidebar-link active" href="/reading/literature/01-EEG-based-detection-of-the-locus-of-auditory-attention-with-convolutional-neural-networks.html" aria-label="01. 基于EEG的卷积神经网络听觉注意力定位检测"><!--[--><i class="vp-icon iconfont icon-boke" sizing="both"></i><!--]-->01. 基于EEG的卷积神经网络听觉注意力定位检测<!----></a></li></ul></section></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Sports</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-header clickable" type="button"><!----><span class="vp-sidebar-title">Translation</span><span class="vp-arrow end"></span></button><!----></section></li><li><a class="route-link auto-link vp-sidebar-link" href="/intro.html" aria-label="介绍页"><!--[--><i class="vp-icon iconfont icon-circle-info" sizing="both"></i><!--]-->介绍页<!----></a></li></ul><!----></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!----><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><i class="vp-icon iconfont icon-boke" sizing="height"></i>01. 基于EEG的卷积神经网络听觉注意力定位检测</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon" name="author"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="page-author-item">Ran</span></span><span property="author" content="Ran"></span></span><span class="page-original-info">原创</span><span class="page-date-info" aria-label="写作日期📅" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon" name="calendar"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span data-allow-mismatch="text">2025/11/23</span><meta property="datePublished" content="2025-11-23T11:39:12.000Z"></span><span class="page-pageview-info" aria-label="访问量🔢" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon eye-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="eye icon" name="eye"><path d="M992 512.096c0-5.76-.992-10.592-1.28-11.136-.192-2.88-1.152-8.064-2.08-10.816-.256-.672-.544-1.376-.832-2.08-.48-1.568-1.024-3.104-1.6-4.32C897.664 290.112 707.104 160 512 160c-195.072 0-385.632 130.016-473.76 322.592-1.056 2.112-1.792 4.096-2.272 5.856a55.512 55.512 0 00-.64 1.6c-1.76 5.088-1.792 8.64-1.632 7.744-.832 3.744-1.568 11.168-1.568 11.168-.224 2.272-.224 4.032.032 6.304 0 0 .736 6.464 1.088 7.808.128 1.824.576 4.512 1.12 6.976h-.032c.448 2.08 1.12 4.096 1.984 6.08.48 1.536.992 2.976 1.472 4.032C126.432 733.856 316.992 864 512 864c195.136 0 385.696-130.048 473.216-321.696 1.376-2.496 2.24-4.832 2.848-6.912.256-.608.48-1.184.672-1.728 1.536-4.48 1.856-8.32 1.728-8.32l-.032.032c.608-3.104 1.568-7.744 1.568-13.28zM512 672c-88.224 0-160-71.776-160-160s71.776-160 160-160 160 71.776 160 160-71.776 160-160 160z"></path></svg><span id="ArtalkPV" class="vp-pageview waline-pageview-count" data-path="/reading/literature/01-EEG-based-detection-of-the-locus-of-auditory-attention-with-convolutional-neural-networks.html" data-page-key="/reading/literature/01-EEG-based-detection-of-the-locus-of-auditory-attention-with-convolutional-neural-networks.html">...</span></span><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon" name="timer"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 25 分钟</span><meta property="timeRequired" content="PT25M"></span><span class="page-category-info" aria-label="分类🌈" data-balloon-pos="up"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="category icon" name="category"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><!--[--><span class="page-category-item color2 clickable" role="navigation">reading</span><span class="page-category-item color8 clickable" role="navigation">literature</span><!--]--><meta property="articleSection" content="reading,literature"></span><!----></div><hr></div><!----><div class="" vp-content><!----><div id="markdown-content"><p><strong>EEG-based detection of the locus of auditory attention with convolutional neural networks</strong></p><figure><img src="/assets/image-20251123114219611-D89kLo7f.png" alt="题目及作者" tabindex="0" loading="lazy"><figcaption>题目及作者</figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>In a multi-speaker scenario, the human auditory system is able to attend to one particular speaker of interest and ignore the others. It has been demonstrated that it is possible to use electroencephalography (EEG) signals to infer to which speaker someone is attending by relating the neural activity to the speech signals. However, classifying auditory attention within a short time interval remains the main challenge. We present a convolutional neural network-based approach to extract the locus of auditory attention (left/right) without knowledge of the speech envelopes. Our results show that it is possible to decode the locus of attention within 1–2 s, with a median accuracy of around 81%. These results are promising for neuro-steered noise suppression in hearing aids, in particular in scenarios where per-speaker envelopes are unavailable.</p><p>在多说话者场景中，人类的听觉系统能够专注于某个感兴趣的特定说话者并忽略其他人。研究表明，通过将神经活动与语音信号关联，可以利用脑电（EEG）信号推断听者正在关注哪位说话者。然而，在短时间隔内对听觉注意进行分类仍然是主要挑战。我们提出了一种基于卷积神经网络的方法，用来提取听觉注意的位置（左/右），且无需语音包络。结果显示，在1–2秒内解码注意位置是可行的，准确率约为81%。这些结果在助听设备中神经引导噪声抑制方面具有前景，尤其是在无法获取每位说话者语音包络的情况下。</p><h2 id="_1-introduction" tabindex="-1"><a class="header-anchor" href="#_1-introduction"><span>1. Introduction</span></a></h2><ul><li><p><strong>现象：</strong></p><p>cocktail party problem 鸡尾酒会问题 （Cherry EC. 1953. Some experiments on the recognition of speech, with one and with two ears. The Journal of the Acoustical Society of America 25:975–979. DOI: <a href="https://doi.org/10.1121/1.1907229%EF%BC%89" target="_blank" rel="noopener noreferrer">https://doi.org/10.1121/1.1907229）</a></p></li><li><p><strong>涉及人群：</strong></p><p>听力损失人群和老年人更难分辨。</p></li><li><p><strong>问题：</strong></p><p>助听设备现有的策略是根据说话人音量或者听者面对的方向，应用效果不佳。</p></li><li><p><strong>解决方法：</strong></p><p>auditory attention decoding (AAD) 听觉注意力解码：神经活动 →解码 听觉注意</p></li></ul><h3 id="_1-1-线性解码-vs-非线性解码问题" tabindex="-1"><a class="header-anchor" href="#_1-1-线性解码-vs-非线性解码问题"><span>1.1 线性解码 VS 非线性解码问题</span></a></h3><p><strong>线性解码</strong></p><ul><li><p><strong>常见方法：</strong></p><p>step 1: 刺激重建（用大脑活动解码并重建刺激语音的包络）;</p><p>step 2：相关性分析（将重建的包络与原始刺激包络进行相关分析，相关性最高的那个包络属于所关注的说话者。）</p><p>(O’Sullivan JA, Power AJ, Mesgarani N, Rajaram S, Foxe JJ, Shinn-Cunningham BG, Slaney M, Shamma SA, Lalor EC. 2015. Attentional selection in a cocktail party environment can be decoded from Single-Trial EEG. Cerebral Cortex 25:1697–1706. DOI: <a href="https://doi.org/10.1093/cercor/bht355" target="_blank" rel="noopener noreferrer">https://doi.org/10.1093/cercor/bht355</a>, PMID: 24429136)</p><p>( Pasley BN, David SV, Mesgarani N, Flinker A, Shamma SA, Crone NE, Knight RT, Chang EF. 2012. Reconstructing speech from human auditory cortex. PLOS Biology 10:e1001251. DOI: <a href="https://doi.org/10.1371/" target="_blank" rel="noopener noreferrer">https://doi.org/10.1371/</a> journal.pbio.1001251, PMID: 22303281)</p></li><li><p><strong>其他方法：</strong></p><p>forward modeling approach;</p><p>predicting EEG from the auditory stimulus;</p><p>( Akram S, Presacco A, Simon JZ, Shamma SA, Babadi B. 2016. Robust decoding of selective auditory attention from MEG in a competing-speaker environment via state-space modeling. NeuroImage 124:906–917. DOI: <a href="https://doi.org/10.1016/j.neuroimage.2015.09.048" target="_blank" rel="noopener noreferrer">https://doi.org/10.1016/j.neuroimage.2015.09.048</a>, PMID: 26436490)</p><p>(Alickovic E, Lunner T, Gustafsson F. 2016. A system identification approach to determining listening attention from EEG signals. 24th European Signal Processing Conference (EUSIPCO) 31–35.)</p><p>canonical correlation analysis (CCA)-based methods;</p><p>( de Cheveigne´ A, Wong DDE, Di Liberto GM, Hjortkjær J, Slaney M, Lalor E. 2018. Decoding the auditory brain with canonical component analysis. NeuroImage 172:206–216. DOI: <a href="https://doi.org/10.1016/j.neuroimage" target="_blank" rel="noopener noreferrer">https://doi.org/10.1016/j.neuroimage</a>. 2018.01.033, PMID: 29378317)</p><p>Bayesian state-space modeling.</p><p>( Miran S, Akram S, Sheikhattar A, Simon JZ, Zhang T, Babadi B. 2018. Real-Time tracking of selective auditory attention from M/EEG: a bayesian filtering approach. Frontiers in Neuroscience 12:262. DOI: <a href="https://doi.org/" target="_blank" rel="noopener noreferrer">https://doi.org/</a> 10.3389/fnins.2018.00262, PMID: 29765298)</p></li></ul><p><strong>非线性解码</strong></p><p>人类听觉系统<strong>非线性</strong>特点（ Faure P, Korn H. 2001. Is there Chaos in the brain? I. concepts of nonlinear dynamics and methods of investigation. Comptes Rendus De l’Acade´mie Des Sciences- Series III- Sciences De La Vie 324:773–793. DOI: <a href="https://doi.org/10.1016/S0764-4469(01)01377-4%EF%BC%89" target="_blank" rel="noopener noreferrer">https://doi.org/10.1016/S0764-4469(01)01377-4）</a></p><p>因此采用非线性模型。</p><ul><li><p>feedforward neural network</p><p>( Taillez T, Kollmeier B, Meyer BT. 2017. Machine learning for decoding listeners’ attention from electroencephalography evoked by continuous speech. European Journal of Neuroscience 51:1234–1241. DOI: <a href="https://doi.org/10.1111/ejn.13790" target="_blank" rel="noopener noreferrer">https://doi.org/10.1111/ejn.13790</a>)</p></li><li><p>convolutional neural networks (CNNs) 方法</p><p>优点：双说话人分离效果优于线性方法；</p><p>缺点：长度10s 时效果好，准确率 75–85%，但时长缩短会导致性能下降。</p></li></ul><h3 id="_1-2-决定窗时长问题" tabindex="-1"><a class="header-anchor" href="#_1-2-决定窗时长问题"><span>1.2 决定窗时长问题</span></a></h3><p>由于时长缩短会导致准确率下降，那么该如何平衡decision window 时长和准确率？</p><p>Geirnaert et al., 2020 提出了一种将两种属性结合成单一指标的方法，通过在具有稳健性约束的基于AAD的音量控制系统中寻找最佳权衡点，以最小化预期切换持续时间。通过对每次新的AAD决策使用较小的相对音量变化，可以提高对AAD错误的鲁棒性。</p><p>适合短时但准确率中等的需求。</p><h3 id="_1-3-位置问题" tabindex="-1"><a class="header-anchor" href="#_1-3-位置问题"><span>1.3 位置问题</span></a></h3><p><strong>任务转向：</strong></p><p>重构的语音包络对应哪个语音刺激 →转向 语音刺激的空间位置。</p><p><strong>好处：</strong> 无需干净语音。</p><p><strong>基础：</strong></p><p>最近的研究（ Wolbers et al., 2011; Bednar and Lalor, 2018; Patel et al., 2018; O’Sullivan et al., 2019; Bednar and Lalor, 2020）表明，听觉注意的方向在神经上是编码的，这表明可能从 EEG 中解码出被注意声音的位置或轨迹。</p><p>一些使用脑磁图（MEG）的研究表明，尤其是α波段的功率可以被用来追踪听觉注意力的位置 (Frey et al., 2014; Wo¨stmann et al., 2016)。另有一项使用头皮脑电图（EEG）的研究发现β波段的功率与选择性注意力相关(Gao et al., 2017)。</p><p><strong>目的：</strong></p><p>通过 EEG 和 CNN 解码位置信息，再在分离出来的目的方向上应用 beamformer。</p><hr><h3 id="_1-4-问题" tabindex="-1"><a class="header-anchor" href="#_1-4-问题"><span>1.4 问题</span></a></h3><p><strong>Q1：</strong></p><p>introduction里面，提到了线性解码和非线性解码，为什么CNN 的方法就是非线性解码的，而前面提到的forward modeling approach等就是线性解码呢？跟模型本身有什么联系？</p><p><strong>Deepseek 回答：</strong></p><p>简单来说，判断一个解码方法是线性还是非线性，关键在于<strong>模型是否能够学习和表示输入（e.g. EEG）与输出（e.g.注意方向）之间的非线性关系</strong>。</p><p>下面我们详细拆解一下：</p><p><strong>线性解码模型</strong></p><p>论文中提到的stimulus reconstruction（刺激重建）、forward modeling（前向建模）等方法，其核心都是线性模型。</p><ol><li>工作原理：</li></ol><p>- 它们本质上都是在寻找一个<strong>线性的权重过滤器</strong>。</p><p>- 例如，在刺激重建中，模型尝试找到一组权重，当这些权重与多通道、多时间延迟的EEG信号进行**加权求和（线性组合）**时，能够最好地重建出语音包络。</p><p>- 这个过程可以用一个简单的公式表示：重建的包络 = W * EEG，其中 W 就是线性权重矩阵。</p><ol start="2"><li>为什么是“线性”的？</li></ol><p>- 因为整个变换过程是线性的。无论输入（EEG）多么复杂，输出（重建的包络或预测的EEG）始终是输入的<strong>加权和</strong>。</p><p>- 它无法捕捉像“如果A通道的信号在频率X处出现峰值，同时B通道的信号在频率Y处出现谷值，那么这很可能表示注意在左侧”这样的<strong>复杂、非加性的相互作用</strong>。它只能学习固定的、加性的贡献。</p><ol start="3"><li>局限性：</li></ol><p>- 人脑的听觉处理系统是高度非线性的（如论文引言中引用的Faure &amp; Korn, 2001）。线性模型是对这个复杂系统的一种<strong>简化近似</strong>，虽然有效，但可能无法捕捉到所有可用于解码注意的信息。</p><p><strong>非线性解码模型（本文的CNN方法）</strong></p><p>卷积神经网络是一种典型的<strong>非线性模型</strong>。</p><ol><li>工作原理与非线性激活函数：</li></ol><p>- CNN的关键组成部分是<strong>激活函数</strong>，如本文中使用的 ReLU。</p><p>- ReLU 的函数是 f(x) = max(0, x)。这是一个非常简单的<strong>非线性函数</strong>。<strong>只要模型中包含了任何一个非线性激活函数，整个模型就变成了非线性系统</strong>。</p><p>- 在卷积层之后使用ReLU，意味着模型不再是简单地对输入进行加权平均，而是可以进行<strong>如果这个特征大于 0，就保留它；否则，就抑制它</strong>这样的<strong>非线性操作</strong>。</p><ol start="2"><li>层次化特征提取：</li></ol><p>- CNN通过多层卷积和池化，能够<strong>分层地</strong>、自动地从原始EEG信号中提取特征。</p><p>- 底层可能提取简单的时域或空域特征。</p><p>- 高层则将这些简单特征组合成更复杂的、抽象的特征模式。</p><p>- 这种<strong>特征的层级组合本身就是高度非线性的</strong>。线性模型无法实现这种层次化的特征学习和组合。</p><ol start="3"><li>模型的表达能力：</li></ol><p>- 正是由于这种非线性，CNN可以学习到输入数据中非常复杂和微妙的模式。</p><p>- 在本文的任务中，这意味着CNN可以不依赖于语音包络，直接从EEG信号中发现与“注意左侧”或“注意右侧”相关的、可能遍布在不同频带、不同通道、不同时间点上的<strong>复杂神经标记</strong>。这些标记之间的关系可能是线性模型无法描述的。</p><p><strong>总结与类比</strong></p><table><thead><tr><th>特性</th><th>线性解码模型</th><th>非线性解码模型（CNN）</th></tr></thead><tbody><tr><td><strong>核心操作</strong></td><td>加权求和</td><td>卷积 + <strong>非线性激活</strong> + 池化</td></tr><tr><td><strong>输入-输出关系</strong></td><td>直线/超平面</td><td>复杂的曲线/曲面</td></tr><tr><td><strong>特征学习</strong></td><td>需要人工设计特征（如特定的频带）或隐含在权重中</td><td><strong>自动学习</strong>层次化的特征</td></tr><tr><td><strong>表达能力</strong></td><td>有限，只能捕捉线性关系</td><td>强大，能捕捉复杂非线性关系</td></tr><tr><td><strong>好比</strong></td><td>用一条直线去拟合一堆数据点</td><td>用一个非常灵活的函数去拟合数据点，能抓住更复杂的趋势</td></tr></tbody></table><p><strong>结论：</strong></p><p>论文之所以强调CNN是非线性方法，是因为它的架构（特别是ReLU激活函数）赋予了它捕捉<strong>大脑EEG信号与注意方向之间复杂、非线性映射关系</strong>的能力。这使得它有可能超越线性模型的性能上限，特别是在数据量充足、模式复杂的情况下。这也解释了为什么CNN在<strong>极短的决策窗口（1秒）</strong> 内依然能保持高精度，因为它能更快地捕捉到那些具有判别性的、非线性的神经活动模式。</p><h2 id="_2-materials-and-methods" tabindex="-1"><a class="header-anchor" href="#_2-materials-and-methods"><span>2. Materials and methods</span></a></h2><h3 id="_2-1-数据来源" tabindex="-1"><a class="header-anchor" href="#_2-1-数据来源"><span>2.1 数据来源</span></a></h3><p>数据：16位正常听力被试的脑电数据</p><p>（ Das N, Biesmans W, Bertrand A, Francart T. 2016. The effect of head-related filtering and ear-specific decoding Bias on auditory attention detection. Journal of Neural Engineering 13:056014. DOI: <a href="https://doi.org/10.1088/" target="_blank" rel="noopener noreferrer">https://doi.org/10.1088/</a> 1741-2560/13/5/056014, PMID: 27618842）</p><h3 id="_2-2-数据处理" tabindex="-1"><a class="header-anchor" href="#_2-2-数据处理"><span>2.2 数据处理</span></a></h3><p><strong>处理一：</strong></p><p>EEG 数据经过等波纹 FIR 带通滤波器（equiripple FIR bandpass filter）滤波，并已对其群延迟进行了补偿。</p><ul><li><p>线性模型：被滤波在1到9赫兹之间（线性注意力解码的最佳频率范围）</p></li><li><p>非线性 CNN：1-32Hz (Taillez et al., 2017)</p></li></ul><p>效果：最大通带衰减为0.5dB，而阻带衰减在0–1Hz时为20dB，在32–64Hz时为15dB。</p><p><strong>处理二：下采样。</strong></p><ul><li>线性模型：20Hz</li><li>CNN：128Hz</li></ul><p><strong>处理三：伪迹去除。</strong></p><p>the generic MWF-based removal algorithm（ Somers B, Francart T, Bertrand A. 2018. A generic EEG artifact removal algorithm based on the multi-channel Wiener filter. Journal of Neural Engineering 15:036007. DOI: <a href="https://doi.org/10.1088/1741-2552/aaac92" target="_blank" rel="noopener noreferrer">https://doi.org/10.1088/1741-2552/aaac92</a>, PMID: 29393057）</p><p><strong>处理四：从模型训练角度处理。</strong></p><ol><li><p><strong>数据划分</strong>： 将每个被试的数据分为训练集、验证集和测试集。</p><p>验证集用于在训练过程中调整模型超参数、监控训练过程（如是否过拟合）和进行模型选择。模型不会直接从它这里学习。</p><p><strong>“按被试进行划分”</strong> ，意味着一个被试的所有数据只会出现在训练、验证或测试的其中一个集合中。这避免了<strong>数据泄露</strong>，即如果同一个被试的数据同时出现在训练和测试集中，模型可能会“记住”这个被试的特有模式而非学习通用的脑电模式，导致性能评估不真实。</p></li><li><p><strong>数据增强</strong>： 使用滑动窗口从连续数据中生成大量数据段（样本），重合度 50%；减少信息丢失（避免重要的脑电事件e.g.一个事件相关电位 刚好被窗口边界切断）。</p></li><li><p><strong>数据标准化</strong>： 对每个被试的数据进行缩放，以消除个体差异，但保留通道间的相对信息。</p><p>这一步是针对训练集的操作。每个被试分开来计算。</p><p>针对一个被试：</p><ul><li>10% trimmed mean of the squared samples：先对单个通道的所有数据点取平方值，之后去掉最高10%和最低10%的值，再计算这个通道的平均值（<strong>排除异常值</strong>）。</li><li>因为是 64导的脑电数据，因此64个通道，有64个平均值。再取64个平均值的中位数。因为平均值最开始先取了平方值，因此再把中位数开平方，转换回振幅尺度，获得一个统一的标准化因子。</li><li>用这位被试所有通道的数据点 <strong>除</strong> 这个标准化因子，来达到数据的归一化。</li></ul></li></ol><h3 id="_2-3-cnn" tabindex="-1"><a class="header-anchor" href="#_2-3-cnn"><span>2.3 CNN</span></a></h3><hr><h3 id="问题" tabindex="-1"><a class="header-anchor" href="#问题"><span>问题：</span></a></h3><p><strong>Q2：</strong></p><p>The stimuli were set to <strong>equal root-mean-square intensities</strong> and were perceived as equally loud.</p><p><strong>Deepseek 回答：</strong></p><p><strong>均方根</strong>是一个数学和统计学概念，英文是 <strong>Root Mean Square</strong>。它用来衡量一个变化信号的“有效值”或“平均功率”。</p><ul><li><strong>为什么需要它？</strong> 声波是一种交流信号，它的压力值在正值和负值之间快速振荡，其简单的算术平均值为零，这无法告诉我们声音的实际强度。</li><li><strong>RMS解决了什么问题？</strong> RMS值能更准确地反映一个波动信号（如声音、电流）做功的能力或我们感知到的持续响度。</li></ul><p><strong>计算RMS的步骤（以声压为例）：</strong></p><ol><li><strong>平方</strong>： 将信号每个瞬间的样本值进行平方。这使得所有值都变为正数。</li><li><strong>取平均</strong>： 计算这些平方值在一段时间内的平均值。</li><li><strong>开方根</strong>： 对平均值进行平方根运算。</li></ol><p>最终得到的结果就是<strong>均方根值</strong>，它代表了产生相同热功率的等效直流信号的值。对于声音来说，就是<strong>声压的均方根值</strong>。</p><p><strong>equal root-mean-square intensities</strong> 指的是：将两个或多个声音信号的均方根值调整到相同的水平。</p><p>换句话说，就是<strong>让这些声音具有相同的平均能量或平均响度</strong>。</p><blockquote><p>为什么这个概念如此重要？</p><p>在比较不同声音时，如果我们只比较它们的<strong>峰值</strong>，可能会产生误导。一个峰值很高但持续时间很短的脉冲（如钉钉子声），其平均响度可能远低于一个峰值较低但持续不断的信号（如空调声）。</p></blockquote><p><strong>“等均方根音强”确保了比较的公平性，因为它比较的是声音的整体能量，而不是瞬间的峰值。</strong></p><p>主要应用场景：</p><ol><li><p><strong>音频制作与母带处理</strong>：</p><ul><li>当混音师将多首歌曲制作成一张专辑时，他们会使用“等均方根”响度来匹配所有歌曲的整体音量。这样听众在切换歌曲时就不会觉得某首突然特别响或特别轻。</li><li>在电视广告中，法规经常要求广告的响度必须与节目的响度一致，使用的就是基于RMS的响度标准（如LUFS）。</li></ul></li><li><p><strong>心理声学实验</strong>：</p><ul><li>在科学研究中，如果想比较不同频率或音色对人耳的影响，必须首先将这些声音在“等均方根音强”下进行匹配。这样才能确保被试者反应的差异是由于频率或音色本身引起的，而不是因为某个声音客观上更响。</li></ul></li><li><p><strong>电子设备和扬声器测试</strong>：</p><ul><li>在测试音箱的频率响应时，工程师会输入不同频率的信号，但确保所有频率的RMS电平是相同的。这样，音箱输出的差异就能真实反映其在不同频率上的性能，而不是输入信号的强度差异。</li></ul></li></ol><p><strong>总结</strong></p><p><strong>等均方根音强</strong>是一个核心概念，它意味着<strong>将不同声音的平均能量（或持续响度）标准化到同一水平</strong>。这样做是为了在各种音频应用中进行公平、科学和一致的比较与控制，避免被短暂的峰值所误导。</p><hr><p><strong>Q3:</strong></p><p>head-related transfer function (HRTF) filtering (simulating sound coming from ± 90deg)</p><p><strong>Deepseek 回答：</strong></p><p>好的，这是一个非常专业的概念。我们来详细解释一下 <strong>“头相关传输函数（HRTF）滤波（模拟来自±90度的声音）”</strong>。</p><p>这其实是现代3D音频技术的核心。我们可以把它拆解成几个部分来理解。</p><ol><li><strong>核心概念：头相关传输函数（HRTF）是什么？</strong></li></ol><p>简单来说，<strong>HRTF是一组数学滤镜，它描述了你的头部、躯干、尤其是外耳（耳廓）如何改变从空间中某个点发出的声音，然后才传入你的耳道。</strong></p><p>当一个声音到达你的身体时，会发生以下几件事：</p><ul><li><strong>头部阴影效应</strong>： 如果声音来自你的右侧，它到达右耳的路程更短、更直接。而到达左耳时，需要绕过你的头部，这会造成： <ul><li><strong>时间差</strong>： 声音稍晚一点到达左耳。</li><li><strong>强度差</strong>： 由于头部的阻挡，左耳听到的声音音量会小一些（尤其是高频声音）。</li></ul></li><li><strong>耳廓效应</strong>： 这是最关键的一步。你的耳廓形状非常复杂，声音在进入耳道前会撞击耳廓的不同褶皱并产生反射和共振。这些微小的反射与原始声音混合，极大地改变了声音的频率特性。<strong>你的大脑正是通过解读这些独特的“频谱线索”来判断声音的高度和前后位置。</strong></li></ul><p><strong>HRTF就是所有这些声学效应的一个总和的数学模型。</strong> 它为每个耳朵都提供了一套独特的滤波参数。</p><ol start="2"><li><strong>HRTF滤波是什么？</strong></li></ol><p><strong>HRTF滤波</strong>，就是将一个普通的单声道音频信号（比如一个枪声或一个人的说话声），通过一个数字信号处理过程，分别与<strong>左耳HRTF</strong>和<strong>右耳HRTF</strong>进行卷积运算。</p><p><strong>处理过程如下：</strong></p><ol><li>你有一个单声道声音源，以及一个目标位置（例如，正右方90度）。</li><li>计算机调用对应于“正右方90度”这个位置的HRTF数据。这个数据包含两套滤波器系数：一套给左耳，一套给右耳。</li><li>原始的单声道声音信号分别通过这两套滤波器。</li><li>输出结果是两个信号：一个给左耳耳机，一个给右耳耳机。</li></ol><p>当你的大脑处理这两个经过“加工”的信号时，它会接收到：</p><ul><li><strong>右耳信号</strong>： 相对原始，音量较大。</li><li><strong>左耳信号</strong>： 包含了由头部阻挡造成的延迟、衰减和频谱变化。</li></ul><p>大脑会立刻识别出这些差异，并得出结论：<strong>“这个声音来自我的右边！”</strong></p><ol start="3"><li><strong>模拟来自±90度的声音</strong></li></ol><p>现在我们聚焦于 <strong>±90度</strong> 这个特定场景。</p><ul><li><strong>+90度</strong>： 通常是正右方。</li><li><strong>-90度</strong>： 通常是正左方。</li></ul><p><strong>这是HRTF效果最显著、最容易感知的位置之一。</strong> 原因如下：</p><ul><li><strong>最大化的双耳差异</strong>： 当声源位于正侧方时，它到达一侧耳朵是直接的，而到达另一侧耳朵则需要绕过整个头部。这时，<strong>时间差</strong> 和 <strong>强度差</strong> 都达到了最大值。</li><li><strong>典型的滤波效果</strong>： <ul><li>对于<strong>对侧耳朵</strong>（声源在右，则左耳为对侧），由于头部阴影效应，<strong>高频部分（特别是2000Hz以上）会被显著衰减</strong>，听起来像是被“过滤”了，更闷。</li><li>对于<strong>同侧耳朵</strong>，声音则相对完整。</li></ul></li></ul><p>所以，当工程师说他们在进行“HRTF滤波（模拟来自±90度的声音）”时，他们就是在<strong>刻意地、人为地</strong>为声音信号施加这种最强的双耳不对称性，从而在你的脑中创造一个坚不可摧的、来自正左或正右的听觉图像。</p><p><strong>应用场景</strong></p><ul><li><strong>虚拟现实（VR）和增强现实（AR）</strong>： 让声音物体与视觉物体在3D空间中的位置匹配，极大增强沉浸感。</li><li><strong>3D音频耳机</strong>： 在音乐、电影和游戏中，创造出超越传统立体声的环绕声场。</li><li><strong>科研和听力诊断</strong>： 用于研究空间听觉机制。</li></ul><p><strong>总结</strong></p><p><strong>头相关传输函数（HRTF）滤波（模拟来自±90度的声音）</strong>，其核心流程是：</p><p><strong>普通单声道声音 → 加载对应于正侧方（±90度）的HRTF滤波器 → 分别生成带有强烈时间差、强度差和频谱差异的左、右耳信号 → 通过耳机回放 → 大脑被“欺骗”，清晰地感知到声音来自正左或正右方。</strong></p><p>这是一种利用人类听觉生理和心理声学原理，通过数字信号处理来精准还原或创造三维声场的关键技术。</p><hr><p>Q4:</p><p>The EEG data was filtered with an equiripple FIR bandpass filter and its group delay was compensated for.</p><p><strong>Deepseek 回答：</strong></p><p>好的，这句话描述了脑电图数据处理中一个非常专业和重要的步骤。我们来逐词分解并详细解释它的含义。</p><p>这句话的核心在于两个操作：</p><ul><li><p><strong>使用等波纹FIR带通滤波器对EEG数据进行滤波。</strong></p></li><li><p><strong>对其群延迟进行补偿。</strong></p></li></ul><ol><li>使用等波纹FIR带通滤波器进行滤波</li></ol><ul><li><strong>EEG数据</strong>： 脑电图信号，记录了大脑的电活动。它通常包含我们感兴趣的频率（如Alpha波：8-13 Hz）以及很多不感兴趣的噪声（如工频干扰、肌电信号、基线漂移等）。</li><li><strong>滤波</strong>： 一个“筛选”过程，目的是<strong>保留特定频率范围</strong>的信号，同时<strong>去除其他频率</strong>的成分。</li><li><strong>带通滤波器</strong>： 一种只允许某个特定频率带宽通过的滤波器。例如，一个0.5-40 Hz的带通滤波器会保留Delta、Theta、Alpha、Beta波，而滤除极低频的漂移和极高频的噪声。</li><li><strong>FIR滤波器</strong>： 这是一种数字滤波器的类型。 <ul><li><strong>优点</strong>： 它是<strong>线性相位</strong>的，这意味着滤波器对所有频率成分造成的<strong>时间延迟是相同的</strong>。这个特性至关重要，是理解第二点的基础。</li></ul></li><li><strong>等波纹</strong>： 这是FIR滤波器的一种设计方法。它试图在通带和阻带之间进行一种优化： <ul><li><strong>通带</strong>： 想要保留的频率范围。等波纹设计确保通带内的信号幅度波动非常小。</li><li><strong>阻带</strong>： 想要抑制的频率范围。等波纹设计确保阻带内的信号被最大限度地抑制。</li><li>简单说，<strong>等波纹设计能在通带和阻带性能之间取得一个很好的、可控的平衡</strong>，是一种非常优秀和常用的滤波器设计方法。</li></ul></li></ul><p><strong>所以，第一部分的意思是：</strong> 我们使用了一种性能优良、设计精良的数字滤波器，对原始的EEG数据进行了清理，只保留了研究所需的频率成分。</p><ol start="2"><li>群延迟补偿</li></ol><p>这是最关键也最难理解的部分。</p><ul><li><p><strong>群延迟</strong>： 简单来说，它就是<strong>信号通过滤波器后所产生的“时间延迟”</strong>。想象一下，原始信号进入滤波器，处理后的信号要“晚一点”才出来。这个“晚一点”的时间就是群延迟。</p></li><li><p><strong>FIR滤波器的线性相位特性</strong>： 正如前面所说，FIR滤波器的一个巨大优点是，它对所有频率造成的<strong>群延迟是一个常数</strong>。也就是说，一个包含1Hz和10Hz成分的复合信号，经过滤波器后，这两个成分的“迟到”时间是相同的，因此它们之间的相对时间关系保持不变，信号波形不会失真。</p></li><li><p><strong>为什么需要补偿？</strong></p><ul><li>即使信号波形没有失真，但这个<strong>固定的时间延迟是真实存在的</strong>。在数据采集时间轴上，滤波后的信号整体向后偏移了一段。</li><li>对于需要<strong>精确时间定位</strong>的分析来说，这是一个严重问题。例如，在事件相关电位研究中，我们需要精确测量一个刺激出现后特定脑电成分（如P300）的峰值出现在多少毫秒。如果数据因为滤波而延迟了50毫秒，那么所有的时间测量都会出错。</li></ul></li><li><p><strong>如何补偿？</strong></p><ul><li>既然延迟是固定且已知的（可以通过滤波器的阶数计算出来，通常是 <code>N/2</code> 个样本点，其中 <code>N</code> 是滤波器阶数），解决方法就很直接了：</li><li><strong>将滤波后的信号在时间轴上向前移动，移动的量正好等于群延迟的量。</strong></li><li>在数字信号处理中，这通常是通过截取信号中间一段稳定部分，或者直接对信号进行 <code>circular shift</code> 来实现的。</li></ul></li></ul><p><strong>所以，第二部分的意思是：</strong> 我们识别并校正了由滤波过程引入的固定时间延迟，确保处理后的EEG数据在时间轴上与原始事件（如视觉/听觉刺激）精确对齐。</p><p><strong>总结</strong></p><p>整句话 <strong>“The EEG data was filtered with an equiripple FIR bandpass filter and its group delay was compensated for.”</strong> 描述了一个专业、严谨的EEG数据预处理流程：</p><ol><li><strong>滤波</strong>： 使用一种<strong>高性能（等波纹）、不会造成波形失真（FIR线性相位）</strong> 的带通滤波器来净化和聚焦EEG信号。</li><li><strong>补偿</strong>： 主动<strong>校正了该滤波器引入的固定时间延迟</strong>，从而保证了后续分析中<strong>时间信息的精确性</strong>。</li></ol><p>这通常是在进行事件相关电位或任何其他需要精确时序分析的研究前，必须执行的标准步骤。</p><hr><p>Q5:</p><p>In both cases, the maximal bandpass attenuation was 0.5dB while the stopband attenuation was 20dB (at 0–1Hz) and15dB (at 32–64Hz).</p><p><strong>Deepseek 回答：</strong></p><ul><li><strong>通带最大衰减： 0.5 dB</strong><ul><li><strong>通带</strong> 是我们想要保留的频率范围（比如 1-32 Hz）。</li><li><strong>0.5 dB 衰减</strong> 意味着在这个频率范围内，信号的强度最多只会被非常轻微地减弱。</li><li><strong>重要性</strong>： 这确保了所有我们感兴趣的脑电波（如Alpha， Beta波）的振幅在通过滤波器后几乎保持不变，避免了有用的信号被过度削弱。这是一个<strong>高性能滤波器</strong>的标志。</li></ul></li><li><strong>阻带衰减： 20 dB (在 0–1 Hz) 和 15 dB (在 32–64 Hz)</strong><ul><li><strong>阻带</strong> 是我们想要极力抑制的频率范围。</li><li><strong>0-1 Hz 阻带</strong>： 这个范围通常包含非常缓慢的<strong>基线漂移</strong>（如由出汗或身体运动引起）。<strong>20 dB 衰减</strong> 意味着这些频率的噪声强度被减弱到原来的约1/10（<code>10^(-20/20) = 0.1</code>）。这有效地去除了低频漂移。</li><li><strong>32-64 Hz 阻带</strong>： 这个范围包含高频噪声，如<strong>肌电信号</strong>（肌肉活动）和其他环境干扰。<strong>15 dB 衰减</strong> 意味着这些高频噪声被减弱到原来的约1/5.6（<code>10^(-15/20) ≈ 0.18</code>）。</li></ul></li></ul><p><strong>简单比喻</strong>： 这个滤波器就像一个非常精密的筛子：</p><ul><li><strong>通带</strong>： 筛孔大小均匀，想要的颗粒（脑电信号）几乎全部通过（只损失0.5%）。</li><li><strong>阻带</strong>： 筛子边缘密封严密，能有效阻挡灰尘（低频漂移）和大颗粒杂质（高频噪声）。</li></ul><hr><p>Q6:</p><p>After the bandpass filtering, the EEG data was down sampled to 20Hz (linear model) and128Hz (CNN).</p><p><strong>Deepseek 回答：</strong></p><ul><li><strong>什么是降采样？</strong><br> 降低数据的采样率。例如，如果原始数据以500 Hz采集，降采样到128 Hz意味着我们每秒只保留128个数据点，丢弃多余的点。</li><li><strong>为什么这么做？</strong><ol><li><strong>减少计算负担</strong>： 更低的数据量意味着更快的处理速度和更小的存储空间。</li><li><strong>匹配信号的有效信息</strong>： 根据奈奎斯特定理，要无失真地还原一个信号，采样率需要至少是信号最高频率的2倍。经过1-32 Hz的滤波后，信号的最高有效频率是32 Hz。因此： <ul><li><strong>128 Hz</strong> 对于32 Hz的信号是绰绰有余的（128 &gt; 2 * 32）。</li><li><strong>20 Hz</strong> 则是一个更具攻击性的降采样，可能用于只关注非常低频成分的模型。</li></ul></li></ol></li><li><strong>为什么对不同模型使用不同采样率？</strong><ul><li><strong>线性模型</strong>： 可能只关注非常缓慢、稳定的趋势（如慢皮层电位），20 Hz的采样率足以捕捉这些变化，同时极大简化了模型。</li><li><strong>CNN</strong>： 卷积神经网络能够学习更复杂、更细微的时间模式。保留更高的128 Hz采样率可以为CNN提供更丰富的时间细节，以提升其性能。</li></ul></li></ul><hr><p>Q7：</p><p>Artifacts were removed with the generic MWF-based removal algorithm described in Somers et al., 2018.</p><p><strong>Deepseek 回答：</strong></p><ul><li><strong>伪迹</strong>： 指EEG记录中非脑源性的污染，例如： <ul><li><strong>眼电伪迹</strong>： 眨眼、眼球运动。</li><li><strong>肌电伪迹</strong>： 头部、颈部、下巴的肌肉紧张。</li><li><strong>心电伪迹</strong>： 心跳。</li></ul></li><li><strong>MWF算法</strong>： <ul><li>MWF代表 <strong>多变量维纳滤波</strong>。</li><li>它是一种先进的空间滤波技术，可以<strong>同时分离和去除多种类型的伪迹</strong>，而不需要像传统方法（如ICA）那样手动标记噪声成分。</li><li><strong>工作原理</strong>： 它利用多个EEG通道的数据，通过数学方法计算出最优的滤波器，能够最好地将干净的脑电信号与伪迹分离开来。它被认为是高效、自动化的伪迹去除方法。</li></ul></li></ul><p><strong>引用 Somers et al., 2018</strong> 表明研究者遵循了一个已发表的、经过验证的标准方法，增加了研究的可重复性和可靠性。</p><p>英文表达学习：</p><p>The desired result is increased speech intelligibility for the listener.</p></div><!----><!--[--><h2 id="doc-changelog" tabindex="-1"><a href="#doc-changelog" class="header-anchor"><span>更新日志</span></a></h2><div class="vp-changelog-wrapper"><div class="vp-changelog-header"><div class="vp-latest-updated"><span class="vp-changelog-icon"></span><span data-allow-mismatch>2025/11/25 07:54</span></div><div><span class="vp-changelog-menu-icon"></span><span>查看所有更新日志</span></div></div><ul class="vp-changelog-list"><!--[--><li class="vp-changelog-item-commit"><span class="vp-changelog-hash" target="_blank" rel="noreferrer"><code>75d62</code></span><span class="vp-changelog-divider">-</span><span class="vp-changelog-message">fix: Remove unnecessary whitespace in local LLM integration code</span><span class="vp-changelog-date" data-allow-mismatch>于 <time datetime="2025-11-25T07:54:40.000Z">2025/11/25</time></span></li><li class="vp-changelog-item-commit"><span class="vp-changelog-hash" target="_blank" rel="noreferrer"><code>05705</code></span><span class="vp-changelog-divider">-</span><span class="vp-changelog-message">feat: Update literature review on auditory attention decoding with linear and nonlinear methods</span><span class="vp-changelog-date" data-allow-mismatch>于 <time datetime="2025-11-23T09:30:36.000Z">2025/11/23</time></span></li><li class="vp-changelog-item-commit"><span class="vp-changelog-hash" target="_blank" rel="noreferrer"><code>c0b9d</code></span><span class="vp-changelog-divider">-</span><span class="vp-changelog-message">Add literature review on EEG-based detection of auditory attention using CNNs</span><span class="vp-changelog-date" data-allow-mismatch>于 <time datetime="2025-11-23T06:13:54.000Z">2025/11/23</time></span></li><!--]--></ul></div><!--]--><!----></div><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link vp-meta-label" href="https://github.com/AmaraMeng/AmaraMeng.github.io/edit/main/src/reading/literature/01-EEG-based-detection-of-the-locus-of-auditory-attention-with-convolutional-neural-networks.md" aria-label="在 GitHub 上编辑此页" rel="noopener noreferrer" target="_blank"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon" name="edit"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<!----></a></div><div class="vp-meta-item git-info"><!----><div class="contributors"><span class="vp-meta-label">贡献者: </span><!--[--><!--[--><span class="vp-meta-info" title="email: ranmeng9558@gmail.com">AmaraMeng</span><!--]--><!--]--></div></div></footer><!----><div id="comment" class="waline-wrapper vp-comment" vp-comment darkmode="false" style="display:block;"><!----></div><!----><!--]--></main><!--]--><footer class="vp-footer-wrapper" vp-footer><div class="vp-footer">默认页脚</div><div class="vp-copyright">Copyright © 2025 Ran </div></footer></div><!--]--><!--[--><!----><!--[--><!--]--><!--]--><!--]--></div>
    <script type="module" src="/assets/app-CyVX4-jx.js" defer></script>
  </body>
</html>
