import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,d as t,o as e}from"./app-Dr9vOq-S.js";const l="/assets/image-20251215162104785-DFdGWoVx.png",p={};function i(m,s){return e(),n("div",null,[...s[0]||(s[0]=[t('<p><strong>FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching</strong></p><p><strong>Interspeech 2025</strong></p><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="摘要" tabindex="-1"><a class="header-anchor" href="#摘要"><span>摘要</span></a></h2><p>Generative models <strong>have excelled in</strong> audio tasks using approaches such as language models, diffusion, and flow matching. However, existing generative approaches for speech enhancement (SE) face notable challenges: language model-based methods suffer from quantization loss, leading to compromised speaker similarity and intelligibility, while diffusion models require complex training and high inference latency. To address these challenges, we propose FlowSE, a flow-matching-based model for SE. Flow matching learns a continuous transformation between noisy and clean speech distributions in a single pass, significantly reducing inference latency while maintaining high-quality reconstruction. Specifically, FlowSE trains on noisy mel spectrograms and optional character sequences, optimizing a condition flow matching loss with ground-truth mel spectrograms as supervision. It implicitly learns speech’s temporal-spectral structure and text-speech alignment. During inference, FlowSE can operate with or without textual information, achieving impressive results in both scenarios, with further improvements when transcripts are available. Extensive experiments demonstrate that FlowSE significantly outper forms state-of-the-art generative methods, establishing a new paradigm for generative-based SE and demonstrating the potential of flow matching to advance the field. Our code, pre-trained checkpoints, and audio samples are available at <a href="https://github.com/Honee-W/FlowSE/" target="_blank" rel="noopener noreferrer">https://github.com/Honee-W/FlowSE/</a>.<br> Index Terms: flow matching, generative models, speech enhancement</p><p>生成模型在音频任务中表现出色，采用了语言模型、扩散模型和流匹配等方法。然而，现有的语音增强（SE）生成方法仍面临显著挑战：基于语言模型的方法存在量化损失，导致说话人相似性和可懂度受损，而扩散模型则需要复杂的训练且推理延迟高。为了解决这些问题，我们提出了FlowSE，一种基于流匹配的语音增强模型。流匹配通过单次处理学习噪声语音与干净语音分布之间的连续变换，可显著降低推理延迟，同时保持高质量重建。具体而言，FlowSE在噪声梅尔频谱图和可选的字符序列上进行训练，使用真实梅尔频谱图作为监督，优化条件流匹配损失。它隐式地学习语音的时-频结构和文本-语音对齐。在推理过程中，FlowSE可以有文本信息或无文本信息两种模式运行，在两种情况下都能取得出色效果，并且在有文字转录时效果会进一步提升。大量实验表明，FlowSE显著优于现有的生成方法，建立了生成式语音增强的新范式，并展示了流匹配推进该领域发展的潜力。我们的代码、预训练检查点和音频样例可在 <a href="https://github.com/Honee-W/FlowSE/" target="_blank" rel="noopener noreferrer">https://github.com/Honee-W/FlowSE/</a> 获取。</p><h3 id="问题" tabindex="-1"><a class="header-anchor" href="#问题"><span>问题</span></a></h3><h4 id="q1-生成式模型和传统回归式模型" tabindex="-1"><a class="header-anchor" href="#q1-生成式模型和传统回归式模型"><span>Q1：生成式模型和传统回归式模型</span></a></h4><p>在语音增强（SE）里，“传统回归式模型”和“生成式模型”主要差在：它们怎么定义输出、怎么训练、推理时怎么产生结果。</p><p><strong>传统回归式模型（regression / discriminative）</strong></p><p>把 SE 当成一个“确定性的映射问题”：</p><ul><li><strong>形式</strong>：给定带噪输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>，直接预测一个输出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\\hat{y}=f_\\theta(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.1076em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>。<br> 输出通常是： <ul><li>干净语音波形 / 干净频谱（mel、STFT 幅度等）</li><li>或者一个 <strong>mask</strong>（如 ratio mask），再用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>×</mo><mtext>mask</mtext></mrow><annotation encoding="application/x-tex">x \\times \\text{mask}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">mask</span></span></span></span></span> 得到增强结果</li></ul></li><li><strong>训练目标</strong>：让预测结果尽量接近真值 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span>，比如 L1/L2、STFT loss、SI-SDR loss 等（本质是“拟合”）。</li><li><strong>推理特点</strong>：<strong>一次前向</strong>就出结果（快、稳定）。</li><li><strong>优缺点直觉</strong>： <ul><li>优点：简单、高效、延迟低。</li><li>局限：它学的是“平均意义下最像”的点估计，遇到噪声很强或信息缺失时，可能会产生<strong>过度平滑</strong>、细节不够自然，或者在不同可能的干净解之间难以“选一个更合理的”。</li></ul></li></ul><p>一句话：<strong>回归式模型 = 直接预测一个最可能/最接近的答案。</strong></p><blockquote><p>训练损失：<strong>L1/L2、STFT loss、SI-SDR loss</strong>。它们都是“让输出更接近干净语音”的误差度量，但关注点不同。</p><h2 id="l1-l2-loss-点对点误差" tabindex="-1"><a class="header-anchor" href="#l1-l2-loss-点对点误差"><span>L1 / L2 loss（点对点误差）</span></a></h2><ul><li><strong>L1（MAE）</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>  </mtext><mi mathvariant="normal">∥</mi><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>−</mo><mi>y</mi><msub><mi mathvariant="normal">∥</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\\; \\| \\hat{y}-y \\|_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">∥</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span><br> 更“抗异常值”，倾向保留尖峰/细节，听感上有时比 L2 不那么糊。</li><li><strong>L2（MSE）</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>  </mtext><mi mathvariant="normal">∥</mi><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>−</mo><mi>y</mi><msubsup><mi mathvariant="normal">∥</mi><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\\; \\| \\hat{y}-y \\|_2^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mord">∥</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mord"><span class="mord">∥</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4519em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2481em;"><span></span></span></span></span></span></span></span></span></span><br> 惩罚大误差更狠，容易得到“平均化”的解，可能更平滑。</li></ul><p>用在 SE 里：可以在<strong>波形</strong>上算，也可以在<strong>谱/梅尔谱</strong>上算。</p><h2 id="stft-loss-频域一致性损失" tabindex="-1"><a class="header-anchor" href="#stft-loss-频域一致性损失"><span>STFT loss（频域一致性损失）</span></a></h2><p>核心：不只比波形点对点，而是比较 <strong>短时傅里叶变换（STFT）</strong> 后的差异，尤其能约束 <strong>频谱形状</strong> 。</p><p>常见写法是组合：</p><ul><li><strong>谱幅度（magnitude）差</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∥</mi><mtext> </mtext><mi mathvariant="normal">∣</mi><mtext>STFT</mtext><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mo>−</mo><mi mathvariant="normal">∣</mi><mtext>STFT</mtext><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mtext> </mtext><mi mathvariant="normal">∥</mi></mrow><annotation encoding="application/x-tex">\\|\\,|\\text{STFT}(\\hat{y})|-|\\text{STFT}(y)|\\,\\|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∥</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord text"><span class="mord">STFT</span></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord text"><span class="mord">STFT</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∥</span></span></span></span></li><li><strong>log 幅度差</strong>：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∥</mi><mi>log</mi><mo>⁡</mo><mi mathvariant="normal">∣</mi><mtext>STFT</mtext><mo stretchy="false">(</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mo>−</mo><mi>log</mi><mo>⁡</mo><mi mathvariant="normal">∣</mi><mtext>STFT</mtext><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∥</mi></mrow><annotation encoding="application/x-tex">\\|\\log|\\text{STFT}(\\hat{y})|-\\log|\\text{STFT}(y)|\\|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∥</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord text"><span class="mord">STFT</span></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mclose">)</span><span class="mord">∣</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">∣</span><span class="mord text"><span class="mord">STFT</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord">∣∥</span></span></span></span></li></ul><p>很多工作还用 <strong>multi-resolution STFT loss</strong>：用多个窗长/hop 同时算，兼顾细节与整体。</p><p>直觉：更贴近人耳对“频率结构/谐波”的敏感性，比纯 L1/L2 更不容易“音色跑偏”。</p><h2 id="si-sdr-loss-尺度不变的信号失真比" tabindex="-1"><a class="header-anchor" href="#si-sdr-loss-尺度不变的信号失真比"><span>SI-SDR loss（尺度不变的信号失真比）</span></a></h2><p>SI-SDR 是评价/训练里很常见的语音分离/增强指标，衡量：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\\hat{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span></span></span></span> 在<strong>允许整体增益缩放</strong>的情况下，和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> 有多一致。</p><p>做法直觉：</p><ol><li>先把 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\\hat{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span></span></span></span> 投影到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> 的方向（找一个最佳缩放系数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span>）</li><li>把投影部分当“目标”，剩下当“失真/噪声”</li><li>比它们的能量比（dB）</li></ol><p>训练时通常用 <strong><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mtext>SI-SDR</mtext></mrow><annotation encoding="application/x-tex">-\\text{SI-SDR}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord">−</span><span class="mord text"><span class="mord">SI-SDR</span></span></span></span></span></strong> 当 loss（越大越好，所以取负号最小化）。</p><p>直觉：更关注“整体波形结构/可懂度相关的失真”，而不纠结于纯粹的幅度标定。</p></blockquote><p><strong>生成式模型（generative）</strong></p><p>把 SE 当成“从条件分布里采样”的问题：</p><ul><li><p><strong>形式</strong>：学习条件分布 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>y</mi><mo>∣</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_\\theta(y\\mid x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∣</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>，即“给定带噪语音 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span>，干净语音 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> 可能有哪些样子”。<br> 推理时不是直接算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>，而是<strong>从模型里生成/采样</strong>一个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span>。</p></li><li><p><strong>训练目标</strong>：不只是做点对点拟合，而是让模型学会这个分布。不同生成式方法的训练目标不同：</p><blockquote><ol><li><strong>扩散模型（Diffusion）：反复“擦干净”</strong></li></ol><p>把干净语音想成一张清晰照片。扩散的生成过程像这样：</p><ul><li><strong>训练时</strong>：故意把干净语音一步步加噪，模型学习“在第 t 步怎么把噪声去掉一点点”。</li><li><strong>推理时</strong>：从“很乱的噪声”开始，<strong>一步一步去噪</strong>，比如 50、100 步……每一步都小幅变干净，最后得到增强语音。</li></ul><p>直觉：<strong>像拿橡皮擦一点点擦干净</strong>。效果常很好，但<strong>步数多就慢</strong>。</p><ol start="2"><li><strong>Flow Matching / Normalizing Flow：学一条“连续的清洗路线”</strong></li></ol><p>这类方法不是学“一步一步擦”，而是学一条“从噪声走到干净”的<strong>连续路线</strong>：</p><ul><li>你可以想象有一条河流/传送带：把“噪声样子”平滑地“搬运”到“干净样子”。</li><li>模型学的是：在任意时刻应该朝哪个方向走、走多快（就像速度/方向的导航）。</li><li><strong>推理时</strong>：用一个 ODE 求解器（常微分方程 ODE, Ordinary Differential Equation），沿着这条路线走一小段一小段（通常 <strong>10–20 步</strong>左右），就到干净结果。</li></ul><p>直觉：<strong>像跟着导航走一条平滑的路线到目的地</strong>。通常比扩散需要的步数少，所以更快。</p><ol start="3"><li><strong>LM + 离散 token：先“变成文字码”，再生成码，最后还原</strong></li></ol><p>这条路线会先把语音变成离散符号（token），类似把音频“压缩成一串编号”：</p><ul><li><strong>第一步：量化/编码</strong><br> 把语音切成一块块，每块用一个编号表示（token）。</li><li><strong>第二步：用语言模型生成 token 序列</strong><br> 像做文本生成那样，预测下一个 token 是什么，从而生成“干净 token 序列”。</li><li><strong>第三步：解码还原成语音</strong><br> 把 token 序列再还原成波形/频谱。</li></ul><p>直觉：<strong>先把音乐变成“简谱编号”，再生成更好的编号，再演奏出来</strong>。<br> 缺点是：这一步“变成编号”会丢掉一些细节（量化损失），可能影响音质/说话人相似度，很容易“像换了个人/口齿糊”。</p></blockquote></li><li><p><strong>推理特点</strong>：通常需要“采样过程”（多步迭代或 ODE 求解），所以历史上常比回归慢。</p></li><li><p><strong>优缺点</strong>：</p><ul><li>优点：更擅长生成<strong>自然细节</strong>、处理“不确定/缺失信息”，在主观听感上可能更好。</li><li>局限：采样带来额外复杂度；有些路线（如离散 token）还会引入量化损失。</li></ul></li></ul><p>一句话：<strong>生成式模型 = 学“可能性空间”，推理时通过采样生成一个合理的答案。</strong></p><h2 id="_1-introduction" tabindex="-1"><a class="header-anchor" href="#_1-introduction"><span>1. Introduction</span></a></h2><p><strong>传统确定性方法缺点</strong></p><p>在具有挑战的场景下<strong>自然度</strong> 不够，而<strong>生成式</strong> 更有潜力做高保真语音增强。</p><blockquote><p><strong>把“自然度”具体化：</strong></p><ul><li>不能只把噪声压下去，还得避免“金属音/机器人音/失真”；</li><li>这往往和模型有没有学到“干净语音的分布结构”有关。</li></ul></blockquote><hr><p><strong>两条主要生成式路线的缺陷</strong></p><p>语言模型 LM 路线：量化过程信息丢失 → artifact（伪影） → 影响说话人相似度和可懂度。 FlowSE Efficient and HighQualit…<br> 扩散 diffusion 路线：迭代随机去噪 → 计算重、延迟高。</p><hr><p><strong>flow matching 优点</strong></p><p>高效的“一次式（one-shot）”生成范式：它学习一个连续的“速度场（velocity field）”，把简单的噪声分布转换到复杂的数据分布。</p><blockquote><p><strong>“simple noise distribution”</strong> 通常就是高斯噪声（很好采样、数学简单）。</p><p><strong>“complex data distribution”</strong> 就是真实语音（或语音的 mel 频谱）在空间里的分布：结构复杂、带强先验（谐波、共振峰、语音节律等）。</p><p><strong>“velocity field（速度场）”</strong> 是一个函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>z</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">v_\\theta(z,t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span>：告诉你“当前状态 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span></span></span></span> 在时间 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 应该往哪里走、走多快”。</p><p><strong>“one-shot”</strong> 在这里更像“训练目标是直接学这个连续速度场”，不是像扩散那样围绕随机去噪链条设计很多步的噪声日程/采样；但注意：<strong>推理仍然可能要走多步（用 ODE solver 积分）</strong>，只是通常步数少很多。</p></blockquote><p><strong>high fidelity（音质/细节） + fast sampling（推理快）</strong></p><hr><p><strong>FlowSE 架构</strong></p><p><strong>rectified flow matching + DiT backbone 组合</strong></p><ul><li><strong>flow matching</strong>：核心是学一个速度场 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">v_\\theta(\\cdot)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span>，用 ODE 把起点分布推到终点分布。</li><li><strong>DiT backbone</strong>：DiT 就是用 Transformer（注意力）当主干来预测这个速度场。</li></ul><p><strong>要抓住的点</strong>：Flow matching 决定了“训练/采样范式”，DiT 决定了“用什么神经网络来实现 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">v_\\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>”</p><p><strong>训练输入：</strong> noisy mel + 可选 transcript</p><p><strong>优化：</strong> condition flow matching loss</p><p><strong>FlowSE 优点</strong></p><ul><li><p>不像以前用 U-Net 或 VAE，Transformer 更能抓<strong>长距离依赖</strong>，从而有助于保留说话人身份。</p><blockquote><p>语音的结构经常是<strong>长程的</strong>：</p><ul><li>时间上：一句话里前后几百毫秒甚至几秒的能量/韵律/发音位置有联系</li><li>频率上：谐波结构会跨多个频带相关</li></ul><p>作者的观点是：Transformer 的自注意力更擅长把这些远距离关系“连起来”，从而更好重建复杂语音模式。</p></blockquote></li><li><p>flow matching 学习的是一个连续变换/速度场（一个模型搞定），不走 <strong>LM 的离散 token</strong>（避免量化损失），也不走 <strong>diffusion 的长链迭代</strong>（避免很多步的去噪采样）</p></li></ul><hr><p>作者总结说：FlowSE 是一个 <strong>rectified flow matching + DiT backbone</strong> 的框架，目标是<strong>高保真</strong>且<strong>推理更快</strong>，并且强调能<strong>保留说话人身份</strong>；实验上改善质量、可懂度、说话人相似度，同时降低延迟。</p><hr><div class="hint-container info"><p class="hint-container-title">思考三个问题：</p><p><strong>快</strong>：到底快多少？用什么指标量化？</p><p><strong>好</strong>：好在哪里？是主观质量、还是可懂度、还是说话人保持？</p><p><strong>为什么</strong>：这些优势是来自 <strong>flow matching</strong> 还是来自 <strong>DiT</strong> 还是来自 <strong>mel+vocoder</strong> 的工程分解？</p></div><h2 id="_2-related-work" tabindex="-1"><a class="header-anchor" href="#_2-related-work"><span>2. Related Work</span></a></h2><h3 id="_2-1-生成式语音增强" tabindex="-1"><a class="header-anchor" href="#_2-1-生成式语音增强"><span>2.1 生成式语音增强</span></a></h3><p>传统 vs 生成式：</p><ul><li>传统（CNN/RNN 等）主要做“确定性重建”，即输入 noisy，网络直接输出一个 clean（点估计）。</li><li>生成式模型学习 clean speech 的“分布”，因此对未知噪声的泛化更强。</li></ul><p>早期生成式缺点：VAE / GAN</p><ul><li><strong>VAE</strong>：有潜在建模优势，但常“过平滑”，不够锐利/高保真。</li><li><strong>GAN</strong>：感知质量可能更好，但训练不稳定、可能训崩/不稳定。</li></ul><p>近年两大生成式主流：LM token 路线、diffusion 路线</p><ul><li><p><strong>LM + 离散 token</strong>：编码成 token 再用 LM 预测，但量化会丢信息，产生 artifact，伤害说话人相似度和可懂度。FlowSE Efficient and HighQualit…</p></li><li><p><strong>Diffusion</strong>：随机去噪迭代，极端噪声下强，但计算重、推理慢，不利实时。</p></li></ul><h3 id="_2-2-生成式建模的流匹配" tabindex="-1"><a class="header-anchor" href="#_2-2-生成式建模的流匹配"><span>2.2 生成式建模的流匹配</span></a></h3>',52)])])}const c=a(p,[["render",i]]),h=JSON.parse('{"path":"/reading/literature/03-FlowSE-Efficient-and-High-Quality-Speech-Enhancement-via-Flow-Matching.html","title":"03. FlowSE：通过流匹配实现高效高质量语音增强","lang":"zh-CN","frontmatter":{"title":"03. FlowSE：通过流匹配实现高效高质量语音增强","icon":"boke","date":"2025-12-15T15:32:12.000Z","author":"Ran","category":["reading","literature"],"isOriginal":true,"sticky":false,"star":false,"article":true,"timeline":true,"image":false,"navbar":true,"sidebarIcon":true,"comment":true,"lastUpdated":true,"editLink":true,"backToTop":true,"toc":true,"description":"FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching Interspeech 2025 摘要 Generative models have excelled in audio tasks using approaches such as language mode...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"03. FlowSE：通过流匹配实现高效高质量语音增强\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-12-15T15:32:12.000Z\\",\\"dateModified\\":\\"2025-12-15T13:55:24.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Ran\\"}]}"],["meta",{"property":"og:url","content":"https://pythiaroot.com/reading/literature/03-FlowSE-Efficient-and-High-Quality-Speech-Enhancement-via-Flow-Matching.html"}],["meta",{"property":"og:site_name","content":"Pythia’s Root"}],["meta",{"property":"og:title","content":"03. FlowSE：通过流匹配实现高效高质量语音增强"}],["meta",{"property":"og:description","content":"FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching Interspeech 2025 摘要 Generative models have excelled in audio tasks using approaches such as language mode..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-12-15T13:55:24.000Z"}],["meta",{"property":"article:author","content":"Ran"}],["meta",{"property":"article:published_time","content":"2025-12-15T15:32:12.000Z"}],["meta",{"property":"article:modified_time","content":"2025-12-15T13:55:24.000Z"}]]},"git":{"createdTime":1765806924000,"updatedTime":1765806924000,"contributors":[{"name":"AmaraMeng","username":"AmaraMeng","email":"ranmeng9558@gmail.com","commits":1,"url":"https://github.com/AmaraMeng"}],"changelog":[{"hash":"5e992b12400f0bea9793db42d5a2a635527e9f89","time":1765806924000,"email":"ranmeng9558@gmail.com","author":"AmaraMeng","message":"Add literature review on FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching"}]},"readingTime":{"minutes":10.53,"words":3160},"filePathRelative":"reading/literature/03-FlowSE-Efficient-and-High-Quality-Speech-Enhancement-via-Flow-Matching.md","excerpt":"<p><strong>FlowSE: Efficient and High-Quality Speech Enhancement via Flow Matching</strong></p>\\n<p><strong>Interspeech 2025</strong></p>\\n<figure><figcaption></figcaption></figure>\\n<h2>摘要</h2>\\n<p>Generative models <strong>have excelled in</strong> audio tasks using approaches such as language models, diffusion, and flow matching. However, existing generative approaches for speech enhancement (SE) face notable challenges: language model-based methods suffer from quantization loss, leading to compromised speaker similarity and intelligibility, while diffusion models require complex training and high inference latency. To address these challenges, we propose FlowSE, a flow-matching-based model for SE. Flow matching learns a continuous transformation between noisy and clean speech distributions in a single pass, significantly reducing inference latency while maintaining high-quality reconstruction. Specifically, FlowSE trains on noisy mel spectrograms and optional character sequences, optimizing a condition flow matching loss with ground-truth mel spectrograms as supervision. It implicitly learns speech’s temporal-spectral structure and text-speech alignment. During inference, FlowSE can operate with or without textual information, achieving impressive results in both scenarios, with further improvements when transcripts are available. Extensive experiments demonstrate that FlowSE significantly outper forms state-of-the-art generative methods, establishing a new paradigm for generative-based SE and demonstrating the potential of flow matching to advance the field. Our code, pre-trained checkpoints, and audio samples are available at <a href=\\"https://github.com/Honee-W/FlowSE/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">https://github.com/Honee-W/FlowSE/</a>.<br>\\nIndex Terms: flow matching, generative models, speech enhancement</p>","autoDesc":true}');export{c as comp,h as data};
